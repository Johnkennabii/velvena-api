# ğŸ¤– Guide de DÃ©ploiement Ollama - Velvena AI

Guide complet pour dÃ©ployer et utiliser Ollama (LLM local) dans l'infrastructure Velvena.

---

## ğŸ“‹ Table des matiÃ¨res

1. [PrÃ©sentation](#prÃ©sentation)
2. [Architecture](#architecture)
3. [DÃ©ploiement sur VPS](#dÃ©ploiement-sur-vps)
4. [Configuration des modÃ¨les](#configuration-des-modÃ¨les)
5. [Utilisation depuis le Frontend](#utilisation-depuis-le-frontend)
6. [Monitoring](#monitoring)
7. [Optimisations](#optimisations)
8. [Troubleshooting](#troubleshooting)

---

## ğŸ¯ PrÃ©sentation

### Qu'est-ce qu'Ollama ?

Ollama est un serveur d'IA open-source qui permet d'exÃ©cuter des modÃ¨les LLM (Large Language Models) localement sur votre infrastructure.

### Pourquoi Ollama pour Velvena ?

- âœ… **ConfidentialitÃ©** : Les donnÃ©es restent sur votre serveur
- âœ… **CoÃ»t** : Pas de frais d'API externes (OpenAI, Claude, etc.)
- âœ… **ContrÃ´le** : MaÃ®trise totale du modÃ¨le utilisÃ©
- âœ… **Performance** : Latence rÃ©duite (pas d'appel rÃ©seau externe)

### ModÃ¨les recommandÃ©s (7.6 GB RAM)

| ModÃ¨le | Taille | RAM requise | Usage |
|--------|--------|-------------|-------|
| `llama3.2:1b` | 1.3 GB | ~2 GB | GÃ©nÃ©ration de texte rapide |
| `phi3:mini` | 2.3 GB | ~3 GB | Ã‰quilibre performance/qualitÃ© |
| `gemma2:2b` | 1.6 GB | ~2.5 GB | GÃ©nÃ©ration de contrats |
| `qwen2.5:0.5b` | 397 MB | ~1 GB | Ultra-lÃ©ger, rÃ©ponses courtes |

---

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Frontend   â”‚
â”‚  (React)    â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚ HTTPS
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     Nginx        â”‚
â”‚  Reverse Proxy   â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   API Backend    â”‚  â† Authentification JWT
â”‚   (Express)      â”‚  â† Logging & MÃ©triques
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚ HTTP (interne)
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     Ollama       â”‚
â”‚   (Port 11434)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**SÃ©curitÃ© :**
- âŒ Ollama n'est **PAS** exposÃ© publiquement
- âœ… AccÃ¨s uniquement via l'API backend (JWT requis)
- âœ… Logs et mÃ©triques pour tous les appels AI

---

## ğŸš€ DÃ©ploiement sur VPS

### Ã‰tape 1 : PrÃ©parer les fichiers

Les fichiers suivants ont Ã©tÃ© modifiÃ©s :

```
docker-compose.yml          â† Service Ollama ajoutÃ©
src/controllers/aiController/aiController.ts  â† Nouveau contrÃ´leur
src/routes/aiRoutes.ts      â† Nouvelles routes
src/utils/metrics.ts        â† MÃ©triques AI
src/server.ts              â† Routes enregistrÃ©es
```

### Ã‰tape 2 : Commit et Push

```bash
# Sur votre machine locale
cd /Users/johnkennabii/Documents/velvena

# Ajouter tous les fichiers
git add docker-compose.yml
git add src/controllers/aiController/
git add src/routes/aiRoutes.ts
git add src/utils/metrics.ts
git add src/server.ts
git add OLLAMA_DEPLOYMENT_GUIDE.md

# Committer
git commit -m "feat: add Ollama AI service integration

- Add Ollama service to docker-compose with memory limits
- Create AI proxy controller with JWT authentication
- Add AI routes for chat and completion
- Add AI metrics (requests counter, duration histogram)
- Ollama exposed only internally (127.0.0.1:11434)
- Resource limits: 3GB max, 2GB reserved

ğŸ¤– Generated with [Claude Code](https://claude.com/claude-code)

Co-Authored-By: Claude <noreply@anthropic.com>"

# Push
git push
```

### Ã‰tape 3 : DÃ©ployer sur le VPS

```bash
# Connexion au serveur
ssh root@VOTRE_IP_SERVEUR
cd /opt/velvena

# Pull les changements
git pull origin main

# DÃ©marrer Ollama
docker compose up -d ollama

# Suivre les logs
docker compose logs -f ollama
```

### Ã‰tape 4 : TÃ©lÃ©charger un modÃ¨le LLM

```bash
# Se connecter au container Ollama
docker compose exec ollama bash

# TÃ©lÃ©charger phi3:mini (recommandÃ© pour 8GB RAM)
ollama pull phi3:mini

# OU tÃ©lÃ©charger llama3.2:1b (plus lÃ©ger)
ollama pull llama3.2:1b

# Lister les modÃ¨les installÃ©s
ollama list

# Quitter le container
exit
```

### Ã‰tape 5 : Rebuild et redÃ©marrer l'API

```bash
# Rebuild l'API avec les nouvelles routes
docker compose build api

# RedÃ©marrer l'API
docker compose up -d api

# VÃ©rifier les logs
docker compose logs api --tail=50 | grep -i "ollama\|ai"
```

### Ã‰tape 6 : VÃ©rifier l'installation

```bash
# 1. VÃ©rifier qu'Ollama tourne
docker compose ps ollama

# 2. Tester directement Ollama (depuis le serveur)
curl http://localhost:11434/api/tags

# 3. Tester via l'API backend
curl -X POST https://api.velvena.fr/ai/generate \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer YOUR_JWT_TOKEN" \
  -d '{
    "model": "phi3:mini",
    "prompt": "Ã‰cris une description courte pour une robe de soirÃ©e Ã©lÃ©gante.",
    "stream": false
  }'
```

---

## ğŸ“¦ Configuration des modÃ¨les

### TÃ©lÃ©charger plusieurs modÃ¨les

```bash
docker compose exec ollama bash

# ModÃ¨les lÃ©gers (< 2GB)
ollama pull llama3.2:1b       # 1.3 GB - Rapide
ollama pull qwen2.5:0.5b      # 397 MB - Ultra-lÃ©ger
ollama pull gemma2:2b         # 1.6 GB - Bon Ã©quilibre

# ModÃ¨les moyens (2-3GB)
ollama pull phi3:mini         # 2.3 GB - RecommandÃ©
ollama pull mistral:7b-instruct-q2_K  # 2.7 GB - QuantisÃ©

# Lister tous les modÃ¨les
ollama list

exit
```

### Supprimer un modÃ¨le

```bash
docker compose exec ollama ollama rm MODEL_NAME
```

### Espace disque occupÃ©

```bash
# Voir l'espace utilisÃ© par Ollama
docker compose exec ollama du -sh /root/.ollama

# Voir l'espace du volume Docker
docker system df -v | grep ollama_data
```

---

## ğŸ’» Utilisation depuis le Frontend

### 1. Configuration Frontend

CrÃ©er un service API pour Ollama :

```typescript
// src/api/endpoints/ai.ts
import { apiClient } from "../apiClient";

export interface GenerateRequest {
  model: string;
  prompt: string;
  stream?: boolean;
}

export interface ChatMessage {
  role: "user" | "assistant" | "system";
  content: string;
}

export interface ChatRequest {
  model: string;
  messages: ChatMessage[];
  stream?: boolean;
}

export const generateCompletion = async (data: GenerateRequest) => {
  const response = await apiClient.post("/ai/generate", data);
  return response.data;
};

export const chat = async (data: ChatRequest) => {
  const response = await apiClient.post("/ai/chat", data);
  return response.data;
};

export const listModels = async () => {
  const response = await apiClient.get("/ai/models");
  return response.data;
};
```

### 2. Exemple d'utilisation - GÃ©nÃ©ration de description

```typescript
// src/components/DressForm.tsx
import { generateCompletion } from "../../api/endpoints/ai";
import { useState } from "react";

const DressForm = () => {
  const [description, setDescription] = useState("");
  const [loading, setLoading] = useState(false);

  const generateDescription = async (dressType: string, color: string) => {
    setLoading(true);
    try {
      const result = await generateCompletion({
        model: "phi3:mini",
        prompt: `Ã‰cris une description Ã©lÃ©gante et professionnelle pour une robe de ${dressType} de couleur ${color}. Maximum 2 phrases.`,
        stream: false,
      });

      setDescription(result.response);
    } catch (error) {
      console.error("Erreur gÃ©nÃ©ration AI:", error);
    } finally {
      setLoading(false);
    }
  };

  return (
    <div>
      <button onClick={() => generateDescription("soirÃ©e", "rouge")} disabled={loading}>
        {loading ? "GÃ©nÃ©ration..." : "GÃ©nÃ©rer une description"}
      </button>
      {description && <p>{description}</p>}
    </div>
  );
};
```

### 3. Exemple d'utilisation - Chat

```typescript
// src/components/AIChat.tsx
import { chat, ChatMessage } from "../../api/endpoints/ai";
import { useState } from "react";

const AIChat = () => {
  const [messages, setMessages] = useState<ChatMessage[]>([]);
  const [input, setInput] = useState("");
  const [loading, setLoading] = useState(false);

  const sendMessage = async () => {
    if (!input.trim()) return;

    const userMessage: ChatMessage = { role: "user", content: input };
    setMessages([...messages, userMessage]);
    setInput("");
    setLoading(true);

    try {
      const result = await chat({
        model: "phi3:mini",
        messages: [...messages, userMessage],
        stream: false,
      });

      const assistantMessage: ChatMessage = {
        role: "assistant",
        content: result.message.content,
      };
      setMessages((prev) => [...prev, assistantMessage]);
    } catch (error) {
      console.error("Erreur chat AI:", error);
    } finally {
      setLoading(false);
    }
  };

  return (
    <div>
      <div className="messages">
        {messages.map((msg, i) => (
          <div key={i} className={msg.role}>
            {msg.content}
          </div>
        ))}
      </div>
      <input
        value={input}
        onChange={(e) => setInput(e.target.value)}
        onKeyPress={(e) => e.key === "Enter" && sendMessage()}
        disabled={loading}
      />
      <button onClick={sendMessage} disabled={loading}>
        {loading ? "..." : "Envoyer"}
      </button>
    </div>
  );
};
```

---

## ğŸ“Š Monitoring

### MÃ©triques Prometheus

Les mÃ©triques suivantes sont automatiquement collectÃ©es :

```
# Nombre de requÃªtes AI (par statut et modÃ¨le)
ai_requests_total{status="success|error", model="phi3:mini"}

# DurÃ©e des requÃªtes AI (par modÃ¨le)
ai_request_duration_seconds{model="phi3:mini"}
```

### Dashboard Grafana

CrÃ©er un dashboard avec ces requÃªtes PromQL :

```promql
# Nombre de requÃªtes AI par minute
rate(ai_requests_total[1m])

# Taux d'erreur AI
rate(ai_requests_total{status="error"}[5m]) / rate(ai_requests_total[5m])

# Latence p95 des requÃªtes AI
histogram_quantile(0.95, rate(ai_request_duration_seconds_bucket[5m]))

# Top 3 modÃ¨les les plus utilisÃ©s
topk(3, sum by(model)(rate(ai_requests_total[5m])))
```

### Logs

```bash
# Voir les logs Ollama
docker compose logs -f ollama

# Voir les logs API contenant "AI"
docker compose logs api | grep -i "ai completion\|ai chat"

# Voir les erreurs AI
docker compose logs api | grep -i "ai.*error"
```

---

## âš¡ Optimisations

### 1. Limiter la mÃ©moire Ollama

DÃ©jÃ  configurÃ© dans `docker-compose.yml` :

```yaml
deploy:
  resources:
    limits:
      memory: 3G    # Maximum 3GB
    reservations:
      memory: 2G    # RÃ©servÃ© 2GB
```

### 2. Limiter les modÃ¨les chargÃ©s

```yaml
environment:
  OLLAMA_MAX_LOADED_MODELS: "1"  # Un seul modÃ¨le en mÃ©moire
```

### 3. Optimiser les prompts

```typescript
// âŒ Mauvais : prompt trop long
const prompt = "Ã‰cris un essai de 10 pages sur l'histoire de la mode franÃ§aise...";

// âœ… Bon : prompt court et prÃ©cis
const prompt = "Ã‰cris une description courte (2 phrases) pour une robe de soirÃ©e rouge.";
```

### 4. Utiliser le cache

Les modÃ¨les gardent le contexte en cache. RÃ©utilisez le mÃªme modÃ¨le pour de meilleures performances.

---

## ğŸ”§ Troubleshooting

### Ollama ne dÃ©marre pas

```bash
# VÃ©rifier les logs
docker compose logs ollama --tail=100

# VÃ©rifier la mÃ©moire disponible
free -h

# RedÃ©marrer Ollama
docker compose restart ollama
```

### "Model not found"

```bash
# Se connecter au container
docker compose exec ollama bash

# Lister les modÃ¨les installÃ©s
ollama list

# Pull le modÃ¨le manquant
ollama pull phi3:mini

exit
```

### API retourne 500

```bash
# VÃ©rifier que l'API peut atteindre Ollama
docker compose exec api curl http://ollama:11434/api/tags

# Si Ã©chec, vÃ©rifier le rÃ©seau Docker
docker network inspect velvena_velvena-network | grep ollama
```

### Ollama est lent

```bash
# 1. VÃ©rifier l'utilisation mÃ©moire
docker stats velvena-ollama

# 2. Utiliser un modÃ¨le plus lÃ©ger
docker compose exec ollama ollama pull llama3.2:1b

# 3. VÃ©rifier si d'autres services consomment de la RAM
docker stats
```

### Espace disque insuffisant

```bash
# Voir l'espace utilisÃ©
df -h
docker system df

# Supprimer les anciens modÃ¨les
docker compose exec ollama ollama rm OLD_MODEL

# Nettoyer Docker
docker system prune -a --volumes
```

---

## ğŸ“ Endpoints API disponibles

### POST /ai/generate

GÃ©nÃ©ration de texte simple.

```bash
curl -X POST https://api.velvena.fr/ai/generate \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer YOUR_JWT" \
  -d '{
    "model": "phi3:mini",
    "prompt": "DÃ©cris une robe Ã©lÃ©gante",
    "stream": false
  }'
```

### POST /ai/chat

Conversation multi-tours.

```bash
curl -X POST https://api.velvena.fr/ai/chat \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer YOUR_JWT" \
  -d '{
    "model": "phi3:mini",
    "messages": [
      {"role": "user", "content": "Bonjour, aide-moi Ã  dÃ©crire une robe"}
    ],
    "stream": false
  }'
```

### GET /ai/models

Liste des modÃ¨les disponibles.

```bash
curl https://api.velvena.fr/ai/models \
  -H "Authorization: Bearer YOUR_JWT"
```

### GET /ai/models/:model

Informations sur un modÃ¨le spÃ©cifique.

```bash
curl https://api.velvena.fr/ai/models/phi3:mini \
  -H "Authorization: Bearer YOUR_JWT"
```

---

## ğŸ¯ Cas d'usage Velvena

1. **GÃ©nÃ©ration de descriptions de robes** - CrÃ©er automatiquement des descriptions marketing
2. **Assistance client** - RÃ©pondre aux questions frÃ©quentes
3. **GÃ©nÃ©ration de contrats** - PrÃ©-remplir des clauses standards
4. **Traduction** - Traduire les descriptions en plusieurs langues
5. **Suggestions de prix** - Analyser le marchÃ© et suggÃ©rer des prix

---

**Documentation gÃ©nÃ©rÃ©e pour Velvena v1.0**
*DerniÃ¨re mise Ã  jour : DÃ©cembre 2025*
